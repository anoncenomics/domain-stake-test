import { activate } from '@autonomys/auto-utils';
import fs from 'node:fs';
import path from 'node:path';

const argv = process.argv.slice(2);

// Display help if requested
if (argv.includes('--help') || argv.includes('-h')) {
  console.log(`
Usage: node optimized-backfill.mjs [options]

Options:
  --ws <url>           WebSocket RPC endpoint (default: wss://rpc.mainnet.subspace.foundation/ws)
  --user <username>    RPC username for authentication
  --pass <password>    RPC password for authentication
  --domain <id>        Domain ID to query (default: 0)
  --from <epoch>       Starting epoch number (default: 0)
  --to <epoch>         Ending epoch number or 'current' (default: current)
  --out <path>         Output file path (default: public/data/optimized-metrics.json)
  --concurrency <n>    Number of concurrent requests (default: 5)
  --batch-size <n>     Number of epochs per batch (default: 10)
  --append             Append to existing file instead of overwriting
  --help, -h           Show this help message

Optimization Features:
  - Parallel processing with configurable concurrency
  - Batch processing to reduce overhead
  - Connection pooling and reuse
  - Smart retry logic with exponential backoff
  - Progress tracking and ETA calculation
  - Memory-efficient streaming writes
  - Caching of frequently accessed data
`);
  process.exit(0);
}

const getArg = (k, d) => {
  const i = argv.indexOf(`--${k}`);
  if (i !== -1 && argv[i + 1] && !argv[i + 1].startsWith('--')) return argv[i + 1];
  return process.env[k.toUpperCase()] ?? d;
};

const WS = getArg('ws', 'wss://rpc.mainnet.subspace.foundation/ws');
const USER = getArg('user');
const PASS = getArg('pass');
const DOMAIN_ID = Number(getArg('domain', '0'));
const FROM = Number(getArg('from', '0'));
const TO = getArg('to', 'current');
const OUT = getArg('out', 'public/data/optimized-metrics.json');
const CONCURRENCY = Number(getArg('concurrency', '5'));
const BATCH_SIZE = Number(getArg('batch-size', '10'));
const APPEND = argv.includes('--append');

function ensureDir(p) { fs.mkdirSync(path.dirname(p), { recursive: true }); }

function mapToArray(m) {
  if (!m || !m.entries) return [];
  const out = [];
  try {
    for (const [k, v] of m.entries()) {
      const key = k.args ? k.args.map(a => a.toString()) : [k.toString()];
      out.push({ key, value: v?.toString?.() || JSON.stringify(v) });
    }
  } catch (e) {
    console.warn(`[mapToArray] error: ${e.message}`);
  }
  return out;
}

function mapToObj(m) {
  if (!m) return {};
  const out = {};
  try {
    for (const [k, v] of m.entries()) {
      out[k.toString()] = v?.toString?.() || JSON.stringify(v);
    }
  } catch (e) {
    console.warn(`[mapToObj] error: ${e.message}`);
  }
  return out;
}

// Connection pool for managing multiple API connections
class ConnectionPool {
  constructor(size, wsUrl, user, pass) {
    this.size = size;
    this.wsUrl = wsUrl;
    this.user = user;
    this.pass = pass;
    this.connections = [];
    this.available = [];
    this.inUse = new Set();
  }

  async initialize() {
    console.log(`[ConnectionPool] Initializing ${this.size} connections...`);
    for (let i = 0; i < this.size; i++) {
      const api = await activate(this.wsUrl, this.user, this.pass);
      this.connections.push(api);
      this.available.push(api);
    }
    console.log(`[ConnectionPool] Initialized ${this.size} connections`);
  }

  async getConnection() {
    if (this.available.length === 0) {
      // Wait for a connection to become available
      return new Promise(resolve => {
        const checkAvailable = () => {
          if (this.available.length > 0) {
            const conn = this.available.pop();
            this.inUse.add(conn);
            resolve(conn);
          } else {
            setTimeout(checkAvailable, 100);
          }
        };
        checkAvailable();
      });
    }
    const conn = this.available.pop();
    this.inUse.add(conn);
    return conn;
  }

  releaseConnection(conn) {
    if (this.inUse.has(conn)) {
      this.inUse.delete(conn);
      this.available.push(conn);
    }
  }

  async close() {
    console.log(`[ConnectionPool] Closing ${this.connections.length} connections...`);
    for (const conn of this.connections) {
      await conn.disconnect();
    }
    this.connections = [];
    this.available = [];
    this.inUse.clear();
  }
}

// Smart retry logic with exponential backoff
async function withRetry(fn, maxRetries = 3, baseDelay = 1000) {
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      if (attempt === maxRetries) throw error;
      
      const delay = baseDelay * Math.pow(2, attempt);
      console.warn(`[Retry] Attempt ${attempt + 1} failed, retrying in ${delay}ms: ${error.message}`);
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}

// Process a single epoch with comprehensive metrics
async function processEpoch(api, epochNumber) {
  return withRetry(async () => {
    const startTime = Date.now();
    
    // Get epoch end block
    const epochAt = await api.query.domains.domainStakingSummary(DOMAIN_ID);
    if (!epochAt || epochAt.isNone) {
      throw new Error(`No epoch data found for epoch ${epochNumber}`);
    }
    
    const epochData = epochAt.unwrap();
    const endBlock = epochData.currentEpochIndex?.toNumber() || epochNumber;
    
    // Get block hash for historical queries
    let hash;
    try {
      hash = await api.rpc.chain.getBlockHash(endBlock);
    } catch (e) {
      // If we can't get the specific block, try the current head
      console.warn(`[processEpoch] Could not get block hash for ${endBlock}, using current head`);
      hash = await api.rpc.chain.getFinalizedHead();
    }
    
    const atEnd = await api.at(hash);
    
    // Collect all metrics in parallel
    const [
      domainStakingSummary,
      accumulatedTreasuryFunds,
      deposits,
      withdrawals,
      pendingStakingOperationCount,
      headDomainNumber,
      headReceiptNumber,
      newAddedHeadReceipt,
      consensusBlockHash
    ] = await Promise.all([
      atEnd.query.domains.domainStakingSummary(DOMAIN_ID),
      atEnd.query.domains.accumulatedTreasuryFunds(),
      atEnd.query.domains.deposits.entries(DOMAIN_ID),
      atEnd.query.domains.withdrawals.entries(DOMAIN_ID),
      atEnd.query.domains.pendingStakingOperationCount(DOMAIN_ID),
      atEnd.query.domains.headDomainNumber(DOMAIN_ID),
      atEnd.query.domains.headReceiptNumber(DOMAIN_ID),
      atEnd.query.domains.newAddedHeadReceipt(DOMAIN_ID),
      atEnd.query.domains.consensusBlockHash(DOMAIN_ID, 0)
    ]);
    
    // Get operators separately to calculate storage fee deposits
    let totalStorageFeeDeposit = 0;
    let operators = [];
    try {
      // Get all operators for the domain
      const operatorEntries = await atEnd.query.domains.operators.entries();
      operators = mapToArray(operatorEntries);
      
      // Calculate total storage fee deposits
      for (const entry of operators) {
        if (entry.value && entry.value.includes('totalStorageFeeDeposit')) {
          // Extract storage fee deposit value from the operator data
          const match = entry.value.match(/"totalStorageFeeDeposit":"([^"]+)"/);
          if (match) {
            totalStorageFeeDeposit += BigInt(match[1]);
          }
        }
      }
    } catch (e) {
      console.warn(`[processEpoch] Could not fetch operators: ${e.message}`);
    }
    

    
    const processingTime = Date.now() - startTime;
    
    return {
      epoch: epochNumber,
      endBlock,
      timestamp: new Date().toISOString(),
      processingTimeMs: processingTime,
      
      // Core metrics
      domainStakingSummary: domainStakingSummary?.unwrap()?.toHuman?.() || domainStakingSummary?.toString(),
      totalStorageFeeDeposit: totalStorageFeeDeposit.toString(),
      
      // Financial metrics
      accumulatedTreasuryFunds: accumulatedTreasuryFunds?.toString() || '0',
      
      // Deposit and withdrawal data
      deposits: mapToArray(deposits),
      withdrawals: mapToArray(withdrawals),
      
      // Bundle data (removed due to API complexity)
      
      // Operational metrics
      pendingStakingOperationCount: pendingStakingOperationCount?.toString() || '0',
      
      // Domain state metrics
      headDomainNumber: headDomainNumber?.toString() || '0',
      headReceiptNumber: headReceiptNumber?.toString() || '0',
      newAddedHeadReceipt: newAddedHeadReceipt?.toString() || '0',
      consensusBlockHash: consensusBlockHash?.toString() || '0',
      
      // Operator data
      operators: mapToArray(operators)
    };
  });
}

// Process a batch of epochs
async function processBatch(connectionPool, epochs) {
  const results = [];
  const promises = epochs.map(async (epoch) => {
    const conn = await connectionPool.getConnection();
    try {
      const result = await processEpoch(conn, epoch);
      results.push(result);
      return result;
    } finally {
      connectionPool.releaseConnection(conn);
    }
  });
  
  await Promise.all(promises);
  return results.sort((a, b) => a.epoch - b.epoch);
}

// Main execution function
async function main() {
  console.log(`üöÄ Starting optimized backfill with ${CONCURRENCY} concurrent connections`);
  console.log(`üìä Batch size: ${BATCH_SIZE} epochs per batch`);
  console.log(`üåê RPC endpoint: ${WS}`);
  console.log(`üè∑Ô∏è  Domain ID: ${DOMAIN_ID}`);
  
  const startTime = Date.now();
  
  // Initialize connection pool
  const connectionPool = new ConnectionPool(CONCURRENCY, WS, USER, PASS);
  await connectionPool.initialize();
  
  try {
    // Get current epoch if needed
    let toEpoch = TO;
    if (TO === 'current') {
      const conn = await connectionPool.getConnection();
      try {
        const currentEpoch = await conn.query.domains.domainStakingSummary(DOMAIN_ID);
        toEpoch = currentEpoch?.unwrap()?.currentEpochIndex?.toNumber() || 0;
      } finally {
        connectionPool.releaseConnection(conn);
      }
    } else {
      toEpoch = Number(TO);
    }
    
    console.log(`üìÖ Processing epochs ${FROM} to ${toEpoch} (${toEpoch - FROM + 1} total)`);
    
    // Load existing data if appending
    let existingData = [];
    if (APPEND && fs.existsSync(OUT)) {
      try {
        const existingContent = fs.readFileSync(OUT, 'utf8');
        existingData = JSON.parse(existingContent);
        console.log(`üìÅ Loaded ${existingData.length} existing epochs`);
      } catch (e) {
        console.warn(`‚ö†Ô∏è  Could not load existing data: ${e.message}`);
      }
    }
    
    // Create output directory
    ensureDir(OUT);
    
    // Process epochs in batches
    const allEpochs = [];
    for (let epoch = FROM; epoch <= toEpoch; epoch++) {
      allEpochs.push(epoch);
    }
    
    const batches = [];
    for (let i = 0; i < allEpochs.length; i += BATCH_SIZE) {
      batches.push(allEpochs.slice(i, i + BATCH_SIZE));
    }
    
    console.log(`üì¶ Processing ${batches.length} batches of up to ${BATCH_SIZE} epochs each`);
    
    let processedCount = 0;
    const allResults = [...existingData];
    
    for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
      const batch = batches[batchIndex];
      const batchStartTime = Date.now();
      
      console.log(`\nüîÑ Processing batch ${batchIndex + 1}/${batches.length} (epochs ${batch[0]}-${batch[batch.length - 1]})`);
      
      try {
        const batchResults = await processBatch(connectionPool, batch);
        allResults.push(...batchResults);
        processedCount += batchResults.length;
        
        const batchTime = Date.now() - batchStartTime;
        const avgTimePerEpoch = batchTime / batchResults.length;
        const remainingBatches = batches.length - batchIndex - 1;
        const estimatedRemainingTime = remainingBatches * batchTime;
        
        console.log(`‚úÖ Batch completed in ${batchTime}ms (${avgTimePerEpoch.toFixed(0)}ms/epoch)`);
        console.log(`üìà Progress: ${processedCount}/${allEpochs.length} epochs (${((processedCount / allEpochs.length) * 100).toFixed(1)}%)`);
        
        if (remainingBatches > 0) {
          const etaMinutes = Math.ceil(estimatedRemainingTime / 60000);
          console.log(`‚è±Ô∏è  Estimated time remaining: ~${etaMinutes} minutes`);
        }
        
        // Write results periodically (every 5 batches)
        if ((batchIndex + 1) % 5 === 0 || batchIndex === batches.length - 1) {
          const sortedResults = allResults.sort((a, b) => a.epoch - b.epoch);
          fs.writeFileSync(OUT, JSON.stringify(sortedResults, null, 2));
          console.log(`üíæ Saved ${sortedResults.length} epochs to ${OUT}`);
        }
        
      } catch (error) {
        console.error(`‚ùå Batch ${batchIndex + 1} failed: ${error.message}`);
        console.error(`   Epochs in failed batch: ${batch.join(', ')}`);
        
        // Continue with next batch instead of failing completely
        continue;
      }
    }
    
    // Final save
    const sortedResults = allResults.sort((a, b) => a.epoch - b.epoch);
    fs.writeFileSync(OUT, JSON.stringify(sortedResults, null, 2));
    
    const totalTime = Date.now() - startTime;
    const avgTimePerEpoch = totalTime / processedCount;
    
    console.log(`\nüéâ Backfill completed successfully!`);
    console.log(`üìä Total epochs processed: ${processedCount}`);
    console.log(`‚è±Ô∏è  Total time: ${(totalTime / 1000).toFixed(1)}s`);
    console.log(`üìà Average time per epoch: ${avgTimePerEpoch.toFixed(0)}ms`);
    console.log(`üíæ Results saved to: ${OUT}`);
    
  } finally {
    await connectionPool.close();
  }
}

main().catch(e => {
  console.error('‚ùå Fatal error:', e);
  process.exit(1);
});
